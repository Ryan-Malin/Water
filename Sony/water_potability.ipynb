{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>204.890455</td>\n",
       "      <td>20791.318981</td>\n",
       "      <td>7.300212</td>\n",
       "      <td>368.516441</td>\n",
       "      <td>564.308654</td>\n",
       "      <td>10.379783</td>\n",
       "      <td>86.990970</td>\n",
       "      <td>2.963135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.716080</td>\n",
       "      <td>129.422921</td>\n",
       "      <td>18630.057858</td>\n",
       "      <td>6.635246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>592.885359</td>\n",
       "      <td>15.180013</td>\n",
       "      <td>56.329076</td>\n",
       "      <td>4.500656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.099124</td>\n",
       "      <td>224.236259</td>\n",
       "      <td>19909.541732</td>\n",
       "      <td>9.275884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>418.606213</td>\n",
       "      <td>16.868637</td>\n",
       "      <td>66.420093</td>\n",
       "      <td>3.055934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.316766</td>\n",
       "      <td>214.373394</td>\n",
       "      <td>22018.417441</td>\n",
       "      <td>8.059332</td>\n",
       "      <td>356.886136</td>\n",
       "      <td>363.266516</td>\n",
       "      <td>18.436524</td>\n",
       "      <td>100.341674</td>\n",
       "      <td>4.628771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.092223</td>\n",
       "      <td>181.101509</td>\n",
       "      <td>17978.986339</td>\n",
       "      <td>6.546600</td>\n",
       "      <td>310.135738</td>\n",
       "      <td>398.410813</td>\n",
       "      <td>11.558279</td>\n",
       "      <td>31.997993</td>\n",
       "      <td>4.075075</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.584087</td>\n",
       "      <td>188.313324</td>\n",
       "      <td>28748.687739</td>\n",
       "      <td>7.544869</td>\n",
       "      <td>326.678363</td>\n",
       "      <td>280.467916</td>\n",
       "      <td>8.399735</td>\n",
       "      <td>54.917862</td>\n",
       "      <td>2.559708</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.223862</td>\n",
       "      <td>248.071735</td>\n",
       "      <td>28749.716544</td>\n",
       "      <td>7.513408</td>\n",
       "      <td>393.663396</td>\n",
       "      <td>283.651634</td>\n",
       "      <td>13.789695</td>\n",
       "      <td>84.603556</td>\n",
       "      <td>2.672989</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.635849</td>\n",
       "      <td>203.361523</td>\n",
       "      <td>13672.091764</td>\n",
       "      <td>4.563009</td>\n",
       "      <td>303.309771</td>\n",
       "      <td>474.607645</td>\n",
       "      <td>12.363817</td>\n",
       "      <td>62.798309</td>\n",
       "      <td>4.401425</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>118.988579</td>\n",
       "      <td>14285.583854</td>\n",
       "      <td>7.804174</td>\n",
       "      <td>268.646941</td>\n",
       "      <td>389.375566</td>\n",
       "      <td>12.706049</td>\n",
       "      <td>53.928846</td>\n",
       "      <td>3.595017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11.180284</td>\n",
       "      <td>227.231469</td>\n",
       "      <td>25484.508491</td>\n",
       "      <td>9.077200</td>\n",
       "      <td>404.041635</td>\n",
       "      <td>563.885481</td>\n",
       "      <td>17.927806</td>\n",
       "      <td>71.976601</td>\n",
       "      <td>4.370562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.360640</td>\n",
       "      <td>165.520797</td>\n",
       "      <td>32452.614409</td>\n",
       "      <td>7.550701</td>\n",
       "      <td>326.624353</td>\n",
       "      <td>425.383419</td>\n",
       "      <td>15.586810</td>\n",
       "      <td>78.740016</td>\n",
       "      <td>3.662292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.974522</td>\n",
       "      <td>218.693300</td>\n",
       "      <td>18767.656682</td>\n",
       "      <td>8.110385</td>\n",
       "      <td>NaN</td>\n",
       "      <td>364.098230</td>\n",
       "      <td>14.525746</td>\n",
       "      <td>76.485911</td>\n",
       "      <td>4.011718</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.119824</td>\n",
       "      <td>156.704993</td>\n",
       "      <td>18730.813653</td>\n",
       "      <td>3.606036</td>\n",
       "      <td>282.344050</td>\n",
       "      <td>347.715027</td>\n",
       "      <td>15.929536</td>\n",
       "      <td>79.500778</td>\n",
       "      <td>3.445756</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>150.174923</td>\n",
       "      <td>27331.361962</td>\n",
       "      <td>6.838223</td>\n",
       "      <td>299.415781</td>\n",
       "      <td>379.761835</td>\n",
       "      <td>19.370807</td>\n",
       "      <td>76.509996</td>\n",
       "      <td>4.413974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7.496232</td>\n",
       "      <td>205.344982</td>\n",
       "      <td>28388.004887</td>\n",
       "      <td>5.072558</td>\n",
       "      <td>NaN</td>\n",
       "      <td>444.645352</td>\n",
       "      <td>13.228311</td>\n",
       "      <td>70.300213</td>\n",
       "      <td>4.777382</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.347272</td>\n",
       "      <td>186.732881</td>\n",
       "      <td>41065.234765</td>\n",
       "      <td>9.629596</td>\n",
       "      <td>364.487687</td>\n",
       "      <td>516.743282</td>\n",
       "      <td>11.539781</td>\n",
       "      <td>75.071617</td>\n",
       "      <td>4.376348</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.051786</td>\n",
       "      <td>211.049406</td>\n",
       "      <td>30980.600787</td>\n",
       "      <td>10.094796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>315.141267</td>\n",
       "      <td>20.397022</td>\n",
       "      <td>56.651604</td>\n",
       "      <td>4.268429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9.181560</td>\n",
       "      <td>273.813807</td>\n",
       "      <td>24041.326280</td>\n",
       "      <td>6.904990</td>\n",
       "      <td>398.350517</td>\n",
       "      <td>477.974642</td>\n",
       "      <td>13.387341</td>\n",
       "      <td>71.457362</td>\n",
       "      <td>4.503661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.975464</td>\n",
       "      <td>279.357167</td>\n",
       "      <td>19460.398131</td>\n",
       "      <td>6.204321</td>\n",
       "      <td>NaN</td>\n",
       "      <td>431.443990</td>\n",
       "      <td>12.888759</td>\n",
       "      <td>63.821237</td>\n",
       "      <td>2.436086</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.371050</td>\n",
       "      <td>214.496610</td>\n",
       "      <td>25630.320037</td>\n",
       "      <td>4.432669</td>\n",
       "      <td>335.754439</td>\n",
       "      <td>469.914551</td>\n",
       "      <td>12.509164</td>\n",
       "      <td>62.797277</td>\n",
       "      <td>2.560299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>227.435048</td>\n",
       "      <td>22305.567414</td>\n",
       "      <td>10.333918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>554.820086</td>\n",
       "      <td>16.331693</td>\n",
       "      <td>45.382815</td>\n",
       "      <td>4.133423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6.660212</td>\n",
       "      <td>168.283747</td>\n",
       "      <td>30944.363591</td>\n",
       "      <td>5.858769</td>\n",
       "      <td>310.930858</td>\n",
       "      <td>523.671298</td>\n",
       "      <td>17.884235</td>\n",
       "      <td>77.042318</td>\n",
       "      <td>3.749701</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>215.977859</td>\n",
       "      <td>17107.224226</td>\n",
       "      <td>5.607060</td>\n",
       "      <td>326.943978</td>\n",
       "      <td>436.256194</td>\n",
       "      <td>14.189062</td>\n",
       "      <td>59.855476</td>\n",
       "      <td>5.459251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.902476</td>\n",
       "      <td>196.903247</td>\n",
       "      <td>21167.500099</td>\n",
       "      <td>6.996312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>444.478883</td>\n",
       "      <td>16.609033</td>\n",
       "      <td>90.181676</td>\n",
       "      <td>4.528523</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.400302</td>\n",
       "      <td>140.739062</td>\n",
       "      <td>17266.593422</td>\n",
       "      <td>10.056852</td>\n",
       "      <td>328.358241</td>\n",
       "      <td>472.874073</td>\n",
       "      <td>11.256381</td>\n",
       "      <td>56.931906</td>\n",
       "      <td>4.824786</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.514415</td>\n",
       "      <td>198.767351</td>\n",
       "      <td>21218.702871</td>\n",
       "      <td>8.670937</td>\n",
       "      <td>323.596349</td>\n",
       "      <td>413.290450</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>79.847843</td>\n",
       "      <td>5.200885</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.445062</td>\n",
       "      <td>207.926260</td>\n",
       "      <td>33424.768678</td>\n",
       "      <td>8.782147</td>\n",
       "      <td>384.007006</td>\n",
       "      <td>441.785876</td>\n",
       "      <td>13.805902</td>\n",
       "      <td>30.284597</td>\n",
       "      <td>4.184397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>145.768181</td>\n",
       "      <td>13224.935639</td>\n",
       "      <td>7.906445</td>\n",
       "      <td>304.001993</td>\n",
       "      <td>298.990666</td>\n",
       "      <td>12.729525</td>\n",
       "      <td>49.536849</td>\n",
       "      <td>4.004871</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>266.421018</td>\n",
       "      <td>26362.965012</td>\n",
       "      <td>7.700063</td>\n",
       "      <td>395.389490</td>\n",
       "      <td>364.480107</td>\n",
       "      <td>10.348951</td>\n",
       "      <td>53.008381</td>\n",
       "      <td>3.991564</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>148.153061</td>\n",
       "      <td>15193.413474</td>\n",
       "      <td>9.046833</td>\n",
       "      <td>307.011793</td>\n",
       "      <td>563.804743</td>\n",
       "      <td>16.568656</td>\n",
       "      <td>52.676185</td>\n",
       "      <td>6.038185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7.181449</td>\n",
       "      <td>209.625601</td>\n",
       "      <td>15196.229987</td>\n",
       "      <td>5.994679</td>\n",
       "      <td>338.336431</td>\n",
       "      <td>342.111286</td>\n",
       "      <td>7.922598</td>\n",
       "      <td>71.537953</td>\n",
       "      <td>5.088860</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>9.825490</td>\n",
       "      <td>190.756618</td>\n",
       "      <td>19677.892466</td>\n",
       "      <td>6.757541</td>\n",
       "      <td>NaN</td>\n",
       "      <td>452.836235</td>\n",
       "      <td>16.899038</td>\n",
       "      <td>47.081971</td>\n",
       "      <td>2.857472</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10.433291</td>\n",
       "      <td>117.791230</td>\n",
       "      <td>22326.892046</td>\n",
       "      <td>8.161505</td>\n",
       "      <td>307.707509</td>\n",
       "      <td>412.986834</td>\n",
       "      <td>12.890709</td>\n",
       "      <td>65.733478</td>\n",
       "      <td>5.057311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>7.414148</td>\n",
       "      <td>235.044534</td>\n",
       "      <td>32555.852537</td>\n",
       "      <td>6.845952</td>\n",
       "      <td>387.175316</td>\n",
       "      <td>411.983364</td>\n",
       "      <td>10.244815</td>\n",
       "      <td>44.489297</td>\n",
       "      <td>3.160624</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NaN</td>\n",
       "      <td>232.280452</td>\n",
       "      <td>14787.206265</td>\n",
       "      <td>5.474915</td>\n",
       "      <td>NaN</td>\n",
       "      <td>383.981723</td>\n",
       "      <td>12.166937</td>\n",
       "      <td>86.080727</td>\n",
       "      <td>5.029167</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.115817</td>\n",
       "      <td>191.952743</td>\n",
       "      <td>19620.545329</td>\n",
       "      <td>6.060713</td>\n",
       "      <td>323.836384</td>\n",
       "      <td>441.748379</td>\n",
       "      <td>10.966486</td>\n",
       "      <td>49.238231</td>\n",
       "      <td>3.902089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3.641630</td>\n",
       "      <td>183.908722</td>\n",
       "      <td>24752.072460</td>\n",
       "      <td>5.538314</td>\n",
       "      <td>286.059556</td>\n",
       "      <td>456.860096</td>\n",
       "      <td>9.034067</td>\n",
       "      <td>73.594657</td>\n",
       "      <td>3.464353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5.618064</td>\n",
       "      <td>304.235912</td>\n",
       "      <td>17281.975168</td>\n",
       "      <td>6.101084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>399.471566</td>\n",
       "      <td>12.265002</td>\n",
       "      <td>81.588992</td>\n",
       "      <td>2.896547</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NaN</td>\n",
       "      <td>143.453731</td>\n",
       "      <td>19942.273218</td>\n",
       "      <td>5.890755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>427.130671</td>\n",
       "      <td>22.469892</td>\n",
       "      <td>53.124094</td>\n",
       "      <td>2.907564</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>9.267188</td>\n",
       "      <td>198.614395</td>\n",
       "      <td>24683.723566</td>\n",
       "      <td>6.110612</td>\n",
       "      <td>328.077533</td>\n",
       "      <td>396.876949</td>\n",
       "      <td>16.471969</td>\n",
       "      <td>30.383315</td>\n",
       "      <td>4.324005</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>NaN</td>\n",
       "      <td>233.858996</td>\n",
       "      <td>11703.923907</td>\n",
       "      <td>4.599388</td>\n",
       "      <td>309.039320</td>\n",
       "      <td>349.399633</td>\n",
       "      <td>18.338893</td>\n",
       "      <td>42.677465</td>\n",
       "      <td>3.510004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5.331940</td>\n",
       "      <td>194.874065</td>\n",
       "      <td>16658.876503</td>\n",
       "      <td>7.993830</td>\n",
       "      <td>316.675162</td>\n",
       "      <td>335.120398</td>\n",
       "      <td>10.180514</td>\n",
       "      <td>59.572714</td>\n",
       "      <td>4.434820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>7.145772</td>\n",
       "      <td>238.689929</td>\n",
       "      <td>28780.340432</td>\n",
       "      <td>6.814029</td>\n",
       "      <td>385.975650</td>\n",
       "      <td>332.032706</td>\n",
       "      <td>11.093163</td>\n",
       "      <td>66.138045</td>\n",
       "      <td>5.182591</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>9.920691</td>\n",
       "      <td>202.817483</td>\n",
       "      <td>9973.934059</td>\n",
       "      <td>6.882248</td>\n",
       "      <td>337.350529</td>\n",
       "      <td>333.192470</td>\n",
       "      <td>23.917601</td>\n",
       "      <td>71.833624</td>\n",
       "      <td>4.690707</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4.758439</td>\n",
       "      <td>183.349454</td>\n",
       "      <td>21568.428779</td>\n",
       "      <td>4.731349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>403.944168</td>\n",
       "      <td>18.668229</td>\n",
       "      <td>66.912400</td>\n",
       "      <td>4.542801</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5.702926</td>\n",
       "      <td>216.850474</td>\n",
       "      <td>35606.440177</td>\n",
       "      <td>7.184351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>504.638260</td>\n",
       "      <td>16.140790</td>\n",
       "      <td>77.536184</td>\n",
       "      <td>4.137739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>6.953864</td>\n",
       "      <td>209.638293</td>\n",
       "      <td>10575.186281</td>\n",
       "      <td>4.462707</td>\n",
       "      <td>315.606594</td>\n",
       "      <td>391.184315</td>\n",
       "      <td>13.285334</td>\n",
       "      <td>87.390889</td>\n",
       "      <td>3.195710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>10.682966</td>\n",
       "      <td>173.375498</td>\n",
       "      <td>15758.740621</td>\n",
       "      <td>5.570784</td>\n",
       "      <td>307.352586</td>\n",
       "      <td>323.807913</td>\n",
       "      <td>10.090870</td>\n",
       "      <td>78.472784</td>\n",
       "      <td>3.999775</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>NaN</td>\n",
       "      <td>129.890572</td>\n",
       "      <td>34415.853146</td>\n",
       "      <td>6.321929</td>\n",
       "      <td>304.535224</td>\n",
       "      <td>470.329169</td>\n",
       "      <td>18.599410</td>\n",
       "      <td>72.403634</td>\n",
       "      <td>4.405586</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>8.757257</td>\n",
       "      <td>200.191400</td>\n",
       "      <td>21536.224687</td>\n",
       "      <td>4.915101</td>\n",
       "      <td>317.882900</td>\n",
       "      <td>404.717799</td>\n",
       "      <td>13.768323</td>\n",
       "      <td>47.930872</td>\n",
       "      <td>3.626135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ph    Hardness        Solids  Chloramines     Sulfate  \\\n",
       "0         NaN  204.890455  20791.318981     7.300212  368.516441   \n",
       "1    3.716080  129.422921  18630.057858     6.635246         NaN   \n",
       "2    8.099124  224.236259  19909.541732     9.275884         NaN   \n",
       "3    8.316766  214.373394  22018.417441     8.059332  356.886136   \n",
       "4    9.092223  181.101509  17978.986339     6.546600  310.135738   \n",
       "5    5.584087  188.313324  28748.687739     7.544869  326.678363   \n",
       "6   10.223862  248.071735  28749.716544     7.513408  393.663396   \n",
       "7    8.635849  203.361523  13672.091764     4.563009  303.309771   \n",
       "8         NaN  118.988579  14285.583854     7.804174  268.646941   \n",
       "9   11.180284  227.231469  25484.508491     9.077200  404.041635   \n",
       "10   7.360640  165.520797  32452.614409     7.550701  326.624353   \n",
       "11   7.974522  218.693300  18767.656682     8.110385         NaN   \n",
       "12   7.119824  156.704993  18730.813653     3.606036  282.344050   \n",
       "13        NaN  150.174923  27331.361962     6.838223  299.415781   \n",
       "14   7.496232  205.344982  28388.004887     5.072558         NaN   \n",
       "15   6.347272  186.732881  41065.234765     9.629596  364.487687   \n",
       "16   7.051786  211.049406  30980.600787    10.094796         NaN   \n",
       "17   9.181560  273.813807  24041.326280     6.904990  398.350517   \n",
       "18   8.975464  279.357167  19460.398131     6.204321         NaN   \n",
       "19   7.371050  214.496610  25630.320037     4.432669  335.754439   \n",
       "20        NaN  227.435048  22305.567414    10.333918         NaN   \n",
       "21   6.660212  168.283747  30944.363591     5.858769  310.930858   \n",
       "22        NaN  215.977859  17107.224226     5.607060  326.943978   \n",
       "23   3.902476  196.903247  21167.500099     6.996312         NaN   \n",
       "24   5.400302  140.739062  17266.593422    10.056852  328.358241   \n",
       "25   6.514415  198.767351  21218.702871     8.670937  323.596349   \n",
       "26   3.445062  207.926260  33424.768678     8.782147  384.007006   \n",
       "27        NaN  145.768181  13224.935639     7.906445  304.001993   \n",
       "28        NaN  266.421018  26362.965012     7.700063  395.389490   \n",
       "29        NaN  148.153061  15193.413474     9.046833  307.011793   \n",
       "30   7.181449  209.625601  15196.229987     5.994679  338.336431   \n",
       "31   9.825490  190.756618  19677.892466     6.757541         NaN   \n",
       "32  10.433291  117.791230  22326.892046     8.161505  307.707509   \n",
       "33   7.414148  235.044534  32555.852537     6.845952  387.175316   \n",
       "34        NaN  232.280452  14787.206265     5.474915         NaN   \n",
       "35   5.115817  191.952743  19620.545329     6.060713  323.836384   \n",
       "36   3.641630  183.908722  24752.072460     5.538314  286.059556   \n",
       "37   5.618064  304.235912  17281.975168     6.101084         NaN   \n",
       "38        NaN  143.453731  19942.273218     5.890755         NaN   \n",
       "39   9.267188  198.614395  24683.723566     6.110612  328.077533   \n",
       "40        NaN  233.858996  11703.923907     4.599388  309.039320   \n",
       "41   5.331940  194.874065  16658.876503     7.993830  316.675162   \n",
       "42   7.145772  238.689929  28780.340432     6.814029  385.975650   \n",
       "43   9.920691  202.817483   9973.934059     6.882248  337.350529   \n",
       "44   4.758439  183.349454  21568.428779     4.731349         NaN   \n",
       "45   5.702926  216.850474  35606.440177     7.184351         NaN   \n",
       "46   6.953864  209.638293  10575.186281     4.462707  315.606594   \n",
       "47  10.682966  173.375498  15758.740621     5.570784  307.352586   \n",
       "48        NaN  129.890572  34415.853146     6.321929  304.535224   \n",
       "49   8.757257  200.191400  21536.224687     4.915101  317.882900   \n",
       "\n",
       "    Conductivity  Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
       "0     564.308654       10.379783        86.990970   2.963135           0  \n",
       "1     592.885359       15.180013        56.329076   4.500656           0  \n",
       "2     418.606213       16.868637        66.420093   3.055934           0  \n",
       "3     363.266516       18.436524       100.341674   4.628771           0  \n",
       "4     398.410813       11.558279        31.997993   4.075075           0  \n",
       "5     280.467916        8.399735        54.917862   2.559708           0  \n",
       "6     283.651634       13.789695        84.603556   2.672989           0  \n",
       "7     474.607645       12.363817        62.798309   4.401425           0  \n",
       "8     389.375566       12.706049        53.928846   3.595017           0  \n",
       "9     563.885481       17.927806        71.976601   4.370562           0  \n",
       "10    425.383419       15.586810        78.740016   3.662292           0  \n",
       "11    364.098230       14.525746        76.485911   4.011718           0  \n",
       "12    347.715027       15.929536        79.500778   3.445756           0  \n",
       "13    379.761835       19.370807        76.509996   4.413974           0  \n",
       "14    444.645352       13.228311        70.300213   4.777382           0  \n",
       "15    516.743282       11.539781        75.071617   4.376348           0  \n",
       "16    315.141267       20.397022        56.651604   4.268429           0  \n",
       "17    477.974642       13.387341        71.457362   4.503661           0  \n",
       "18    431.443990       12.888759        63.821237   2.436086           0  \n",
       "19    469.914551       12.509164        62.797277   2.560299           0  \n",
       "20    554.820086       16.331693        45.382815   4.133423           0  \n",
       "21    523.671298       17.884235        77.042318   3.749701           0  \n",
       "22    436.256194       14.189062        59.855476   5.459251           0  \n",
       "23    444.478883       16.609033        90.181676   4.528523           0  \n",
       "24    472.874073       11.256381        56.931906   4.824786           0  \n",
       "25    413.290450       14.900000        79.847843   5.200885           0  \n",
       "26    441.785876       13.805902        30.284597   4.184397           0  \n",
       "27    298.990666       12.729525        49.536849   4.004871           0  \n",
       "28    364.480107       10.348951        53.008381   3.991564           0  \n",
       "29    563.804743       16.568656        52.676185   6.038185           0  \n",
       "30    342.111286        7.922598        71.537953   5.088860           0  \n",
       "31    452.836235       16.899038        47.081971   2.857472           0  \n",
       "32    412.986834       12.890709        65.733478   5.057311           0  \n",
       "33    411.983364       10.244815        44.489297   3.160624           0  \n",
       "34    383.981723       12.166937        86.080727   5.029167           0  \n",
       "35    441.748379       10.966486        49.238231   3.902089           0  \n",
       "36    456.860096        9.034067        73.594657   3.464353           0  \n",
       "37    399.471566       12.265002        81.588992   2.896547           0  \n",
       "38    427.130671       22.469892        53.124094   2.907564           0  \n",
       "39    396.876949       16.471969        30.383315   4.324005           0  \n",
       "40    349.399633       18.338893        42.677465   3.510004           0  \n",
       "41    335.120398       10.180514        59.572714   4.434820           0  \n",
       "42    332.032706       11.093163        66.138045   5.182591           0  \n",
       "43    333.192470       23.917601        71.833624   4.690707           0  \n",
       "44    403.944168       18.668229        66.912400   4.542801           0  \n",
       "45    504.638260       16.140790        77.536184   4.137739           0  \n",
       "46    391.184315       13.285334        87.390889   3.195710           0  \n",
       "47    323.807913       10.090870        78.472784   3.999775           0  \n",
       "48    470.329169       18.599410        72.403634   4.405586           0  \n",
       "49    404.717799       13.768323        47.930872   3.626135           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water_df = pd.read_csv(\"water_potability.csv\")\n",
    "water_df.iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3276 entries, 0 to 3275\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   ph               2785 non-null   float64\n",
      " 1   Hardness         3276 non-null   float64\n",
      " 2   Solids           3276 non-null   float64\n",
      " 3   Chloramines      3276 non-null   float64\n",
      " 4   Sulfate          2495 non-null   float64\n",
      " 5   Conductivity     3276 non-null   float64\n",
      " 6   Organic_carbon   3276 non-null   float64\n",
      " 7   Trihalomethanes  3114 non-null   float64\n",
      " 8   Turbidity        3276 non-null   float64\n",
      " 9   Potability       3276 non-null   int64  \n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 256.1 KB\n"
     ]
    }
   ],
   "source": [
    "# Know about the dataset\n",
    "water_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1998\n",
       "1    1278\n",
       "Name: Potability, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_count = water_df[\"Potability\"].value_counts() \n",
    "label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>9.445130</td>\n",
       "      <td>145.805402</td>\n",
       "      <td>13168.529156</td>\n",
       "      <td>9.444471</td>\n",
       "      <td>310.583374</td>\n",
       "      <td>592.659021</td>\n",
       "      <td>8.606397</td>\n",
       "      <td>77.577460</td>\n",
       "      <td>3.875165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>9.024845</td>\n",
       "      <td>128.096691</td>\n",
       "      <td>19859.676476</td>\n",
       "      <td>8.016423</td>\n",
       "      <td>300.150377</td>\n",
       "      <td>451.143481</td>\n",
       "      <td>14.770863</td>\n",
       "      <td>73.778026</td>\n",
       "      <td>3.985251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>NaN</td>\n",
       "      <td>169.974849</td>\n",
       "      <td>23403.637304</td>\n",
       "      <td>8.519730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>475.573562</td>\n",
       "      <td>12.924107</td>\n",
       "      <td>50.861913</td>\n",
       "      <td>2.747313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>6.800119</td>\n",
       "      <td>242.008082</td>\n",
       "      <td>39143.403329</td>\n",
       "      <td>9.501695</td>\n",
       "      <td>187.170714</td>\n",
       "      <td>376.456593</td>\n",
       "      <td>11.432466</td>\n",
       "      <td>73.777275</td>\n",
       "      <td>3.854940</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>7.174135</td>\n",
       "      <td>203.408935</td>\n",
       "      <td>20401.102461</td>\n",
       "      <td>7.681806</td>\n",
       "      <td>287.085679</td>\n",
       "      <td>315.549900</td>\n",
       "      <td>14.533510</td>\n",
       "      <td>74.405616</td>\n",
       "      <td>3.939896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>4.668102</td>\n",
       "      <td>193.681735</td>\n",
       "      <td>47580.991603</td>\n",
       "      <td>7.166639</td>\n",
       "      <td>359.948574</td>\n",
       "      <td>526.424171</td>\n",
       "      <td>13.894419</td>\n",
       "      <td>66.687695</td>\n",
       "      <td>4.435821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>7.808856</td>\n",
       "      <td>193.553212</td>\n",
       "      <td>17329.802160</td>\n",
       "      <td>8.061362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>392.449580</td>\n",
       "      <td>19.903225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.798243</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3273</th>\n",
       "      <td>9.419510</td>\n",
       "      <td>175.762646</td>\n",
       "      <td>33155.578218</td>\n",
       "      <td>7.350233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>432.044783</td>\n",
       "      <td>11.039070</td>\n",
       "      <td>69.845400</td>\n",
       "      <td>3.298875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>5.126763</td>\n",
       "      <td>230.603758</td>\n",
       "      <td>11983.869376</td>\n",
       "      <td>6.303357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>402.883113</td>\n",
       "      <td>11.168946</td>\n",
       "      <td>77.488213</td>\n",
       "      <td>4.708658</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3275</th>\n",
       "      <td>7.874671</td>\n",
       "      <td>195.102299</td>\n",
       "      <td>17404.177061</td>\n",
       "      <td>7.509306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>327.459760</td>\n",
       "      <td>16.140368</td>\n",
       "      <td>78.698446</td>\n",
       "      <td>2.309149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1278 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ph    Hardness        Solids  Chloramines     Sulfate  \\\n",
       "250   9.445130  145.805402  13168.529156     9.444471  310.583374   \n",
       "251   9.024845  128.096691  19859.676476     8.016423  300.150377   \n",
       "252        NaN  169.974849  23403.637304     8.519730         NaN   \n",
       "253   6.800119  242.008082  39143.403329     9.501695  187.170714   \n",
       "254   7.174135  203.408935  20401.102461     7.681806  287.085679   \n",
       "...        ...         ...           ...          ...         ...   \n",
       "3271  4.668102  193.681735  47580.991603     7.166639  359.948574   \n",
       "3272  7.808856  193.553212  17329.802160     8.061362         NaN   \n",
       "3273  9.419510  175.762646  33155.578218     7.350233         NaN   \n",
       "3274  5.126763  230.603758  11983.869376     6.303357         NaN   \n",
       "3275  7.874671  195.102299  17404.177061     7.509306         NaN   \n",
       "\n",
       "      Conductivity  Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
       "250     592.659021        8.606397        77.577460   3.875165           1  \n",
       "251     451.143481       14.770863        73.778026   3.985251           1  \n",
       "252     475.573562       12.924107        50.861913   2.747313           1  \n",
       "253     376.456593       11.432466        73.777275   3.854940           1  \n",
       "254     315.549900       14.533510        74.405616   3.939896           1  \n",
       "...            ...             ...              ...        ...         ...  \n",
       "3271    526.424171       13.894419        66.687695   4.435821           1  \n",
       "3272    392.449580       19.903225              NaN   2.798243           1  \n",
       "3273    432.044783       11.039070        69.845400   3.298875           1  \n",
       "3274    402.883113       11.168946        77.488213   4.708658           1  \n",
       "3275    327.459760       16.140368        78.698446   2.309149           1  \n",
       "\n",
       "[1278 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water_df.loc[water_df.Potability == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHO Standards for the potable water\n",
    "* PH between 6.50 - 8.50 pH\n",
    "* Choramines <= 4 mg/L (CDC)\n",
    "* Conductivity <=400 mueS/cm (WHO)\n",
    "* Trihalomethanes <= 80 ppm (US)\n",
    "* Solids between 500 - 1000 mg/L (WHO)\n",
    "* Organic Cabon < 2 mg/L (US)\n",
    "* Turbidity < 5 NTU (WHO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ph                 491\n",
       "Hardness             0\n",
       "Solids               0\n",
       "Chloramines          0\n",
       "Sulfate            781\n",
       "Conductivity         0\n",
       "Organic_carbon       0\n",
       "Trihalomethanes    162\n",
       "Turbidity            0\n",
       "Potability           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find any null values in the dataset\n",
    "clean_water = water_df.copy()\n",
    "clean_water.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values\n",
    "clean_water.dropna(inplace = True)\n",
    "# clean_water.drop('Solids', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find the mean values of the features\n",
    "# clean_water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null with mean values of each features\n",
    "# clean_water.Trihalomethanes.fillna(1, inplace = True)\n",
    "# clean_water.Sulfate.fillna(129, inplace = True)\n",
    "# clean_water.ph.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ph</th>\n",
       "      <td>0.014530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hardness</th>\n",
       "      <td>-0.001505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Solids</th>\n",
       "      <td>0.040674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chloramines</th>\n",
       "      <td>0.020784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sulfate</th>\n",
       "      <td>-0.015303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conductivity</th>\n",
       "      <td>-0.015496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Organic_carbon</th>\n",
       "      <td>-0.015567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <td>0.009244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turbidity</th>\n",
       "      <td>0.022682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Potability</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Potability\n",
       "ph                 0.014530\n",
       "Hardness          -0.001505\n",
       "Solids             0.040674\n",
       "Chloramines        0.020784\n",
       "Sulfate           -0.015303\n",
       "Conductivity      -0.015496\n",
       "Organic_carbon    -0.015567\n",
       "Trihalomethanes    0.009244\n",
       "Turbidity          0.022682\n",
       "Potability         1.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_water.corr()[[\"Potability\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the features columns into features and target\n",
    "X = clean_water.drop('Potability', axis = 1)\n",
    "y = clean_water[\"Potability\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 45, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated Neural Network Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a method that creates a new Sequential model with hyperparameter options\n",
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
    "    activation = hp.Choice('activation',['relu','tanh', 'sigmoid'])\n",
    "    \n",
    "    # Allow kerastuner to decide number of neurons in first layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
    "        min_value=1,\n",
    "        max_value=30,\n",
    "        step=5), activation=activation, input_dim=9))\n",
    "\n",
    "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 5)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "            min_value=1,\n",
    "            max_value=30,\n",
    "            step=4),\n",
    "            activation=activation))\n",
    "    \n",
    "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kerastuner library\n",
    "import keras_tuner as kt\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    create_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    factor = 3,\n",
    "    max_epochs=20,\n",
    "    hyperband_iterations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 00m 04s]\n",
      "val_accuracy: 0.6724565625190735\n",
      "\n",
      "Best val_accuracy So Far: 0.7394540905952454\n",
      "Total elapsed time: 00h 02m 04s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# # Run the kerastuner search for best hyperparameters\n",
    "tuner.search(X_train_scaled,y_train,epochs=20,validation_data=(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in .\\untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "activation: relu\n",
      "first_units: 26\n",
      "num_layers: 2\n",
      "units_0: 21\n",
      "units_1: 17\n",
      "units_2: 13\n",
      "units_3: 9\n",
      "units_4: 9\n",
      "tuner/epochs: 20\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 4e17b2c70b4c3327c6fcf9e240ff7498\n",
      "Score: 0.7394540905952454\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "activation: relu\n",
      "first_units: 26\n",
      "num_layers: 3\n",
      "units_0: 25\n",
      "units_1: 21\n",
      "units_2: 25\n",
      "units_3: 17\n",
      "units_4: 13\n",
      "tuner/epochs: 7\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.7121587991714478\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "activation: tanh\n",
      "first_units: 26\n",
      "num_layers: 3\n",
      "units_0: 29\n",
      "units_1: 29\n",
      "units_2: 13\n",
      "units_3: 29\n",
      "units_4: 13\n",
      "tuner/epochs: 20\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: e528a4e140ca787aa419020339ef7a4e\n",
      "Score: 0.7096773982048035\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "activation: relu\n",
      "first_units: 11\n",
      "num_layers: 1\n",
      "units_0: 13\n",
      "units_1: 9\n",
      "units_2: 29\n",
      "units_3: 1\n",
      "units_4: 25\n",
      "tuner/epochs: 20\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: d1aee9d6c3f1613b163761e5b79b0c18\n",
      "Score: 0.7022332549095154\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "activation: relu\n",
      "first_units: 26\n",
      "num_layers: 2\n",
      "units_0: 21\n",
      "units_1: 17\n",
      "units_2: 13\n",
      "units_3: 9\n",
      "units_4: 9\n",
      "tuner/epochs: 7\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.6972704529762268\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "activation: relu\n",
      "first_units: 26\n",
      "num_layers: 3\n",
      "units_0: 25\n",
      "units_1: 21\n",
      "units_2: 25\n",
      "units_3: 17\n",
      "units_4: 13\n",
      "tuner/epochs: 20\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: d3be2aa43ff04b2a2c73fa3724826318\n",
      "Score: 0.6972704529762268\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "activation: tanh\n",
      "first_units: 26\n",
      "num_layers: 2\n",
      "units_0: 21\n",
      "units_1: 21\n",
      "units_2: 21\n",
      "units_3: 13\n",
      "units_4: 17\n",
      "tuner/epochs: 20\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 4b42d5d37a58c36b6c19434dee654b56\n",
      "Score: 0.6724565625190735\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "activation: tanh\n",
      "first_units: 21\n",
      "num_layers: 2\n",
      "units_0: 9\n",
      "units_1: 25\n",
      "units_2: 25\n",
      "units_3: 9\n",
      "units_4: 5\n",
      "tuner/epochs: 20\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.6724565625190735\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "activation: tanh\n",
      "first_units: 26\n",
      "num_layers: 4\n",
      "units_0: 25\n",
      "units_1: 13\n",
      "units_2: 1\n",
      "units_3: 5\n",
      "units_4: 21\n",
      "tuner/epochs: 20\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: f2668f45f76c123c2148d009fd0798af\n",
      "Score: 0.6674938201904297\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "activation: tanh\n",
      "first_units: 6\n",
      "num_layers: 4\n",
      "units_0: 5\n",
      "units_1: 9\n",
      "units_2: 17\n",
      "units_3: 1\n",
      "units_4: 25\n",
      "tuner/epochs: 20\n",
      "tuner/initial_epoch: 7\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 105a5b9e7eb948a00aa521a1d1587156\n",
      "Score: 0.6650124192237854\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 11)                110       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 122\n",
      "Trainable params: 122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Create first layer of input\n",
    "model.add(tf.keras.layers.Dense(units = 11, activation = 'relu', input_dim=9))\n",
    "\n",
    "# Create hidden layers \n",
    "# model.add(tf.keras.layers.Dense(units = 17, activation = 'relu'))\n",
    "# model.add(tf.keras.layers.Dense(units = 21, activation = 'relu'))\n",
    "# model.add(tf.keras.layers.Dense(units = 25, activation = 'relu'))\n",
    "\n",
    "# Create ouput layers\n",
    "model.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Structure of sequential model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50/50 [==============================] - 1s 6ms/step - loss: 0.7924 - accuracy: 0.5238 - val_loss: 0.6973 - val_accuracy: 0.6061\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7554 - accuracy: 0.5352 - val_loss: 0.6822 - val_accuracy: 0.6061\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7299 - accuracy: 0.5435 - val_loss: 0.6729 - val_accuracy: 0.6061\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7121 - accuracy: 0.5543 - val_loss: 0.6706 - val_accuracy: 0.5758\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6992 - accuracy: 0.5714 - val_loss: 0.6690 - val_accuracy: 0.5758\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.5790 - val_loss: 0.6676 - val_accuracy: 0.5455\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6822 - accuracy: 0.5784 - val_loss: 0.6687 - val_accuracy: 0.5455\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6757 - accuracy: 0.5784 - val_loss: 0.6700 - val_accuracy: 0.5152\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6707 - accuracy: 0.5740 - val_loss: 0.6707 - val_accuracy: 0.5152\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6657 - accuracy: 0.5778 - val_loss: 0.6735 - val_accuracy: 0.5152\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6618 - accuracy: 0.5917 - val_loss: 0.6722 - val_accuracy: 0.5152\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6584 - accuracy: 0.5911 - val_loss: 0.6731 - val_accuracy: 0.5152\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6548 - accuracy: 0.5968 - val_loss: 0.6721 - val_accuracy: 0.5152\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6518 - accuracy: 0.6032 - val_loss: 0.6699 - val_accuracy: 0.5152\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6489 - accuracy: 0.6089 - val_loss: 0.6686 - val_accuracy: 0.5152\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6461 - accuracy: 0.6114 - val_loss: 0.6682 - val_accuracy: 0.5152\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6432 - accuracy: 0.6133 - val_loss: 0.6656 - val_accuracy: 0.5455\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6408 - accuracy: 0.6184 - val_loss: 0.6640 - val_accuracy: 0.5455\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6383 - accuracy: 0.6235 - val_loss: 0.6633 - val_accuracy: 0.5152\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6361 - accuracy: 0.6273 - val_loss: 0.6600 - val_accuracy: 0.5152\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6334 - accuracy: 0.6311 - val_loss: 0.6605 - val_accuracy: 0.4848\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6312 - accuracy: 0.6375 - val_loss: 0.6583 - val_accuracy: 0.4545\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6291 - accuracy: 0.6368 - val_loss: 0.6591 - val_accuracy: 0.5152\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6270 - accuracy: 0.6406 - val_loss: 0.6578 - val_accuracy: 0.5758\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6251 - accuracy: 0.6419 - val_loss: 0.6559 - val_accuracy: 0.5455\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6235 - accuracy: 0.6470 - val_loss: 0.6591 - val_accuracy: 0.5455\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6217 - accuracy: 0.6502 - val_loss: 0.6570 - val_accuracy: 0.5152\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6199 - accuracy: 0.6502 - val_loss: 0.6569 - val_accuracy: 0.5455\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6183 - accuracy: 0.6552 - val_loss: 0.6546 - val_accuracy: 0.5455\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6174 - accuracy: 0.6552 - val_loss: 0.6554 - val_accuracy: 0.5758\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6155 - accuracy: 0.6540 - val_loss: 0.6570 - val_accuracy: 0.5758\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6143 - accuracy: 0.6641 - val_loss: 0.6557 - val_accuracy: 0.5758\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6131 - accuracy: 0.6590 - val_loss: 0.6578 - val_accuracy: 0.5758\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6115 - accuracy: 0.6610 - val_loss: 0.6580 - val_accuracy: 0.6061\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6103 - accuracy: 0.6641 - val_loss: 0.6574 - val_accuracy: 0.6061\n",
      "Epoch 36/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6095 - accuracy: 0.6616 - val_loss: 0.6585 - val_accuracy: 0.6364\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6080 - accuracy: 0.6686 - val_loss: 0.6564 - val_accuracy: 0.6061\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6070 - accuracy: 0.6635 - val_loss: 0.6578 - val_accuracy: 0.6364\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6060 - accuracy: 0.6673 - val_loss: 0.6597 - val_accuracy: 0.6364\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6046 - accuracy: 0.6679 - val_loss: 0.6595 - val_accuracy: 0.6364\n",
      "Epoch 41/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6037 - accuracy: 0.6673 - val_loss: 0.6575 - val_accuracy: 0.6364\n",
      "Epoch 42/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6026 - accuracy: 0.6711 - val_loss: 0.6593 - val_accuracy: 0.6364\n",
      "Epoch 43/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6015 - accuracy: 0.6781 - val_loss: 0.6544 - val_accuracy: 0.6364\n",
      "Epoch 44/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6009 - accuracy: 0.6743 - val_loss: 0.6575 - val_accuracy: 0.6364\n",
      "Epoch 45/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5998 - accuracy: 0.6756 - val_loss: 0.6579 - val_accuracy: 0.6364\n",
      "Epoch 46/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5990 - accuracy: 0.6800 - val_loss: 0.6584 - val_accuracy: 0.6364\n",
      "Epoch 47/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5984 - accuracy: 0.6813 - val_loss: 0.6571 - val_accuracy: 0.6364\n",
      "Epoch 48/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5973 - accuracy: 0.6825 - val_loss: 0.6609 - val_accuracy: 0.6364\n",
      "Epoch 49/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5964 - accuracy: 0.6794 - val_loss: 0.6605 - val_accuracy: 0.6364\n",
      "Epoch 50/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5959 - accuracy: 0.6825 - val_loss: 0.6592 - val_accuracy: 0.6364\n",
      "Epoch 51/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5949 - accuracy: 0.6832 - val_loss: 0.6613 - val_accuracy: 0.6364\n",
      "Epoch 52/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5940 - accuracy: 0.6883 - val_loss: 0.6631 - val_accuracy: 0.6364\n",
      "Epoch 53/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5936 - accuracy: 0.6832 - val_loss: 0.6640 - val_accuracy: 0.6364\n",
      "Epoch 54/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5931 - accuracy: 0.6857 - val_loss: 0.6619 - val_accuracy: 0.6364\n",
      "Epoch 55/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5921 - accuracy: 0.6857 - val_loss: 0.6619 - val_accuracy: 0.6364\n",
      "Epoch 56/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5913 - accuracy: 0.6876 - val_loss: 0.6601 - val_accuracy: 0.6364\n",
      "Epoch 57/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5904 - accuracy: 0.6933 - val_loss: 0.6599 - val_accuracy: 0.6364\n",
      "Epoch 58/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5899 - accuracy: 0.6895 - val_loss: 0.6623 - val_accuracy: 0.6667\n",
      "Epoch 59/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5893 - accuracy: 0.6902 - val_loss: 0.6617 - val_accuracy: 0.6667\n",
      "Epoch 60/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5890 - accuracy: 0.6940 - val_loss: 0.6594 - val_accuracy: 0.6667\n",
      "Epoch 61/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5882 - accuracy: 0.6946 - val_loss: 0.6601 - val_accuracy: 0.6667\n",
      "Epoch 62/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5879 - accuracy: 0.6921 - val_loss: 0.6625 - val_accuracy: 0.6667\n",
      "Epoch 63/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5874 - accuracy: 0.6952 - val_loss: 0.6625 - val_accuracy: 0.6667\n",
      "Epoch 64/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5869 - accuracy: 0.6940 - val_loss: 0.6646 - val_accuracy: 0.6667\n",
      "Epoch 65/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5860 - accuracy: 0.6946 - val_loss: 0.6644 - val_accuracy: 0.6667\n",
      "Epoch 66/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5856 - accuracy: 0.6933 - val_loss: 0.6632 - val_accuracy: 0.6667\n",
      "Epoch 67/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5851 - accuracy: 0.6959 - val_loss: 0.6638 - val_accuracy: 0.6667\n",
      "Epoch 68/100\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5811 - accuracy: 0.70 - 0s 2ms/step - loss: 0.5847 - accuracy: 0.6978 - val_loss: 0.6677 - val_accuracy: 0.6667\n",
      "Epoch 69/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5841 - accuracy: 0.6984 - val_loss: 0.6659 - val_accuracy: 0.6667\n",
      "Epoch 70/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5834 - accuracy: 0.6990 - val_loss: 0.6637 - val_accuracy: 0.6667\n",
      "Epoch 71/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5833 - accuracy: 0.6984 - val_loss: 0.6663 - val_accuracy: 0.6970\n",
      "Epoch 72/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5828 - accuracy: 0.6978 - val_loss: 0.6654 - val_accuracy: 0.6970\n",
      "Epoch 73/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5824 - accuracy: 0.6978 - val_loss: 0.6635 - val_accuracy: 0.6970\n",
      "Epoch 74/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5821 - accuracy: 0.6978 - val_loss: 0.6655 - val_accuracy: 0.6970\n",
      "Epoch 75/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5811 - accuracy: 0.6990 - val_loss: 0.6656 - val_accuracy: 0.6970\n",
      "Epoch 76/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5808 - accuracy: 0.7016 - val_loss: 0.6630 - val_accuracy: 0.6970\n",
      "Epoch 77/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5802 - accuracy: 0.6971 - val_loss: 0.6655 - val_accuracy: 0.6970\n",
      "Epoch 78/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5796 - accuracy: 0.7016 - val_loss: 0.6652 - val_accuracy: 0.6970\n",
      "Epoch 79/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5797 - accuracy: 0.7003 - val_loss: 0.6659 - val_accuracy: 0.6970\n",
      "Epoch 80/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5791 - accuracy: 0.7054 - val_loss: 0.6694 - val_accuracy: 0.6970\n",
      "Epoch 81/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5787 - accuracy: 0.7010 - val_loss: 0.6684 - val_accuracy: 0.6970\n",
      "Epoch 82/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5783 - accuracy: 0.7029 - val_loss: 0.6673 - val_accuracy: 0.6970\n",
      "Epoch 83/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5782 - accuracy: 0.7054 - val_loss: 0.6636 - val_accuracy: 0.6970\n",
      "Epoch 84/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5778 - accuracy: 0.7022 - val_loss: 0.6681 - val_accuracy: 0.6970\n",
      "Epoch 85/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5775 - accuracy: 0.7029 - val_loss: 0.6620 - val_accuracy: 0.6970\n",
      "Epoch 86/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5771 - accuracy: 0.7048 - val_loss: 0.6590 - val_accuracy: 0.6970\n",
      "Epoch 87/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5766 - accuracy: 0.7035 - val_loss: 0.6621 - val_accuracy: 0.6970\n",
      "Epoch 88/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5758 - accuracy: 0.7022 - val_loss: 0.6617 - val_accuracy: 0.6970\n",
      "Epoch 89/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5756 - accuracy: 0.7054 - val_loss: 0.6648 - val_accuracy: 0.6970\n",
      "Epoch 90/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5752 - accuracy: 0.7060 - val_loss: 0.6626 - val_accuracy: 0.6970\n",
      "Epoch 91/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5747 - accuracy: 0.7041 - val_loss: 0.6635 - val_accuracy: 0.6970\n",
      "Epoch 92/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5747 - accuracy: 0.7060 - val_loss: 0.6616 - val_accuracy: 0.6970\n",
      "Epoch 93/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5741 - accuracy: 0.7035 - val_loss: 0.6603 - val_accuracy: 0.6970\n",
      "Epoch 94/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5738 - accuracy: 0.7016 - val_loss: 0.6635 - val_accuracy: 0.6970\n",
      "Epoch 95/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5732 - accuracy: 0.7073 - val_loss: 0.6654 - val_accuracy: 0.6970\n",
      "Epoch 96/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5730 - accuracy: 0.7054 - val_loss: 0.6636 - val_accuracy: 0.6970\n",
      "Epoch 97/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5725 - accuracy: 0.7041 - val_loss: 0.6627 - val_accuracy: 0.6970\n",
      "Epoch 98/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5723 - accuracy: 0.7086 - val_loss: 0.6613 - val_accuracy: 0.6970\n",
      "Epoch 99/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.7041 - val_loss: 0.6617 - val_accuracy: 0.6970\n",
      "Epoch 100/100\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5721 - accuracy: 0.7073 - val_loss: 0.6621 - val_accuracy: 0.6970\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "# lr = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, name='Adam')\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "fit_model = model.fit(X_train_scaled, y_train, epochs = 100, validation_split=0.02) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 - 0s - loss: 0.5720 - accuracy: 0.7058\n",
      "Tain Loss: 0.5719649791717529, Train Accuracy: 0.7058457732200623\n",
      "13/13 - 0s - loss: 0.5752 - accuracy: 0.7171\n",
      "Test Loss: 0.5751603245735168, Test Accuracy: 0.7171216011047363\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = model.evaluate(X_train_scaled,y_train,verbose=2)\n",
    "print(f\"Tain Loss: {model_loss}, Train Accuracy: {model_accuracy}\")\n",
    "model_loss, model_accuracy = model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Test Loss: {model_loss}, Test Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame containing training history\n",
    "history_df = pd.DataFrame(fit_model.history, index=range(1,len(fit_model.history[\"loss\"])+1))\n",
    "\n",
    "# Plot the accuracy\n",
    "history_df.plot(y=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.5970149253731343\n",
      "Testing Data Score: 0.6401985111662531\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression().fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "print(f\"Training Data Score: {lr.score(X_train_scaled, y_train)}\")\n",
    "print(f\"Testing Data Score: {lr.score(X_test_scaled, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
